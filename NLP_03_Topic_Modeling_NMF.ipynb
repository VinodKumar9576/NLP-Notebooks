{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this Notebook we will apply Topic Modeling using NMF i.e Non Negative Matrix Factorization\n",
    "\n",
    "If we remember the Vh matrix that we get from SVD matrix decomposition has some `negative` values in it, what NMF does it gives only `positive` values inside that Vh matrix also **NMF** results in 2 matrices instead of 3.\n",
    "\n",
    "Suppose we have a dataset V after NMF decomposition we get result as 2 matrices i.e W and H.\n",
    "So $V = WH$, both $W$ and $H$ are `positive` matrices.\n",
    "Also **NMF** is `non-unique` decomposition on the other hand SVD was a unique decomposition.\n",
    "\n",
    "<img src = \"./NMF_rep.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's implement NMF using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will use the same new group data as before.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/biku/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# basic imports\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk import stem\n",
    "\n",
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import linalg\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "# we will only work with 4 categories to keep things simple and easy to understand\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "# we will remove below attributes, as we only want articles text\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "# load the data for train and test mode\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=remove)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034,) (2034,)\n"
     ]
    }
   ],
   "source": [
    "# explore the data little bit\n",
    "\n",
    "# how many data points we have ?\n",
    "print(newsgroups_train.filenames.shape,newsgroups_train.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_names = newsgroups_train.target_names\n",
    "target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article / document : \n",
      "This sounds wonderful, but it seems no one either wants to spend time doing\n",
      "this, or they don't have the power to do so.  For example, I would like\n",
      "to see a comp.graphics architecture like this:\n",
      "\n",
      "comp.graphics.algorithms.2d\n",
      "comp.graphics.algorithms.3d\n",
      "comp.graphics.algorithms.misc\n",
      "comp.graphics.hardware\n",
      "comp.graphics.misc\n",
      "comp.graphics.software/apps\n",
      "\n",
      "However, that is almost overkill.  Something more like this would probably\n",
      "make EVERYONE a lot happier:\n",
      "\n",
      "comp.graphics.programmer\n",
      "comp.graphics.hardware\n",
      "comp.graphics.apps\n",
      "comp.graphics.misc\n",
      "\n",
      "It would be nice to see specialized groups devote to 2d, 3d, morphing,\n",
      "raytracing, image processing, interactive graphics, toolkits, languages,\n",
      "object systems, etc. but these could be posted to a relevant group or\n",
      "have a mailing list organized.\n",
      "\n",
      "That way when someone reads news they don't have to see these subject\n",
      "headings, which are rather disparate:\n",
      "\n",
      "System specific stuff ( should be under comp.sys or comp.os.???.programmer ):\n",
      "\n",
      "\t\"Need help programming GL\"\n",
      "\t\"ModeX programming information?\"\n",
      "\t\"Fast sprites on PC\"\n",
      "\n",
      "Hardware technical stuff:\n",
      "\n",
      "\t\"Speed of Weitek P9000\"\n",
      "\t\"Drivers for SpeedStar 24X\"\n",
      "\n",
      "Applications oriented stuff:\n",
      "\n",
      "\t\"VistaPro 3.0 help\"\n",
      "\t\"How good is 3dStudio?\"\n",
      "\t\"Best image processing program for Amiga\"\n",
      "\n",
      "Programming oriented stuff:\n",
      "\n",
      "\t\"Fast polygon routine needed\"\n",
      "\t\"Good morphing alogirhtm wanted\"\n",
      "\t\"Best depth sort for triangles?\"\n",
      "\t\"Which C++ library to get?\"\n",
      "\n",
      "I wish someone with the power would get a CFD and then a CFV going on\n",
      "this stuff....this newsgroup needs it.        \n",
      "=================================\n",
      "category      : comp.graphics\n"
     ]
    }
   ],
   "source": [
    "# look the some samples\n",
    "\n",
    "idx = np.random.choice(2034)\n",
    "# idx = 0\n",
    "\n",
    "print(f\"article / document : {newsgroups_train.data[idx]}\\\n",
    "        \\n=================================\\ncategory\\\n",
    "      : {target_names[newsgroups_train.target[idx]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 26576)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tf- Idf\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer(stop_words='english')\n",
    "# tf-idf matrix will be normalized and most stop words will be zero\n",
    "vectors_tfidf = vectorizer_tfidf.fit_transform(newsgroups_train.data).todense()\n",
    "vectors_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using sklearn implementation of NMF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 26576)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in NMF we have to choose number of topics , consider this as a hyper-parameter\n",
    "\n",
    "no_of_topics = 5\n",
    "\n",
    "nmf = NMF(n_components = no_of_topics,random_state = 1)\n",
    "\n",
    "W1 = nmf.fit_transform(vectors_tfidf)\n",
    "H1 = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 26576)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26576\n",
      "['detects' 'deter' 'deteriorated' 'deterioration' 'determinant'\n",
      " 'determination' 'determinations' 'determine' 'determined' 'determines']\n"
     ]
    }
   ],
   "source": [
    "vocab = np.array(vectorizer_tfidf.get_feature_names())\n",
    "\n",
    "print(len(vocab))\n",
    "\n",
    "# look at some samples\n",
    "print(vocab[8000:8010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(Vh,vocab,no_of_words):\n",
    "    res = []\n",
    "    for i,v in enumerate(Vh):\n",
    "        # stick the words and v together\n",
    "        vocab_components = zip(vocab,v)\n",
    "        \n",
    "            \n",
    "        # sort the vocab components according to the importance that is captured in v\n",
    "        sorted_components = sorted(vocab_components,key = lambda x:x[1],reverse = True)\\\n",
    "                                                                            [:no_of_words]\n",
    "        for c in sorted_components:\n",
    "            res.append(c[0])\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['people', 'don', 'think', 'just', 'like', 'objective', 'say', 'morality', 'graphics', 'thanks', 'files', 'image', 'file', 'program', 'windows', 'know', 'space', 'nasa', 'launch', 'shuttle', 'orbit', 'moon', 'lunar', 'earth', 'ico', 'bobbe', 'tek', 'beauchaine', 'bronx', 'manhattan', 'sank', 'queens', 'god', 'jesus', 'bible', 'believe', 'christian', 'atheism', 'does', 'belief']\n"
     ]
    }
   ],
   "source": [
    "no_of_words = 8\n",
    "\n",
    "print_topics(H1,vocab,no_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some more points about NMF**\n",
    "- Since it's not an unique decomposition we may not get our original matrix back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
